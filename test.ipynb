{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/whalenlex/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Copyright (c) 2015-present, Facebook, Inc.\n",
    "# All rights reserved.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from functools import partial\n",
    "from torch import Tensor\n",
    "from typing import Optional\n",
    "\n",
    "from timm.models.vision_transformer import VisionTransformer, _cfg\n",
    "from timm.models.registry import register_model\n",
    "from timm.models.layers import trunc_normal_, lecun_normal_\n",
    "\n",
    "from timm.models.layers import DropPath, to_2tuple\n",
    "from timm.models.vision_transformer import _load_weights\n",
    "\n",
    "import math\n",
    "\n",
    "from collections import namedtuple\n",
    "\n",
    "from mamba_ssm.modules.mamba_simple import Mamba\n",
    "from mamba_ssm.utils.generation import GenerationMixin\n",
    "from mamba_ssm.utils.hf import load_config_hf, load_state_dict_hf\n",
    "\n",
    "from rope import *\n",
    "import random\n",
    "\n",
    "try:\n",
    "    from mamba_ssm.ops.triton.layernorm import RMSNorm, layer_norm_fn, rms_norm_fn\n",
    "except ImportError:\n",
    "    RMSNorm, layer_norm_fn, rms_norm_fn = None, None, None\n",
    "\n",
    "\n",
    "__all__ = [\n",
    "    'vim_tiny_patch16_224', 'vim_small_patch16_224', 'vim_base_patch16_224',\n",
    "    'vim_tiny_patch16_384', 'vim_small_patch16_384', 'vim_base_patch16_384',\n",
    "]\n",
    "\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\" 2D Image to Patch Embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=224, patch_size=16, stride=16, in_chans=3, embed_dim=768, norm_layer=None, flatten=True):\n",
    "        super().__init__()\n",
    "        img_size = to_2tuple(img_size)\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.grid_size = ((img_size[0] - patch_size[0]) // stride + 1, (img_size[1] - patch_size[1]) // stride + 1)\n",
    "        self.num_patches = self.grid_size[0] * self.grid_size[1]\n",
    "        self.flatten = flatten\n",
    "\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=stride)\n",
    "        self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        assert H == self.img_size[0] and W == self.img_size[1], \\\n",
    "            f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n",
    "        x = self.proj(x)\n",
    "        if self.flatten:\n",
    "            x = x.flatten(2).transpose(1, 2)  # BCHW -> BNC\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(\n",
    "        self, dim, mixer_cls, norm_cls=nn.LayerNorm, fused_add_norm=False, residual_in_fp32=False,drop_path=0.,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Simple block wrapping a mixer class with LayerNorm/RMSNorm and residual connection\"\n",
    "\n",
    "        This Block has a slightly different structure compared to a regular\n",
    "        prenorm Transformer block.\n",
    "        The standard block is: LN -> MHA/MLP -> Add.\n",
    "        [Ref: https://arxiv.org/abs/2002.04745]\n",
    "        Here we have: Add -> LN -> Mixer, returning both\n",
    "        the hidden_states (output of the mixer) and the residual.\n",
    "        This is purely for performance reasons, as we can fuse add and LayerNorm.\n",
    "        The residual needs to be provided (except for the very first block).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.residual_in_fp32 = residual_in_fp32\n",
    "        self.fused_add_norm = fused_add_norm\n",
    "        self.mixer = mixer_cls(dim)\n",
    "        self.norm = norm_cls(dim)\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        if self.fused_add_norm:\n",
    "            assert RMSNorm is not None, \"RMSNorm import fails\"\n",
    "            assert isinstance(\n",
    "                self.norm, (nn.LayerNorm, RMSNorm)\n",
    "            ), \"Only LayerNorm and RMSNorm are supported for fused_add_norm\"\n",
    "\n",
    "    def forward(\n",
    "        self, hidden_states: Tensor, residual: Optional[Tensor] = None, inference_params=None\n",
    "    ):\n",
    "        r\"\"\"Pass the input through the encoder layer.\n",
    "\n",
    "        Args:\n",
    "            hidden_states: the sequence to the encoder layer (required).\n",
    "            residual: hidden_states = Mixer(LN(residual))\n",
    "        \"\"\"\n",
    "        if not self.fused_add_norm:\n",
    "            if residual is None:\n",
    "                residual = hidden_states\n",
    "            else:\n",
    "                residual = residual + self.drop_path(hidden_states)\n",
    "            \n",
    "            hidden_states = self.norm(residual.to(dtype=self.norm.weight.dtype))\n",
    "            if self.residual_in_fp32:\n",
    "                residual = residual.to(torch.float32)\n",
    "        else:\n",
    "            fused_add_norm_fn = rms_norm_fn if isinstance(self.norm, RMSNorm) else layer_norm_fn\n",
    "            if residual is None:\n",
    "                hidden_states, residual = fused_add_norm_fn(\n",
    "                    hidden_states,\n",
    "                    self.norm.weight,\n",
    "                    self.norm.bias,\n",
    "                    residual=residual,\n",
    "                    prenorm=True,\n",
    "                    residual_in_fp32=self.residual_in_fp32,\n",
    "                    eps=self.norm.eps,\n",
    "                )\n",
    "            else:\n",
    "                hidden_states, residual = fused_add_norm_fn(\n",
    "                    self.drop_path(hidden_states),\n",
    "                    self.norm.weight,\n",
    "                    self.norm.bias,\n",
    "                    residual=residual,\n",
    "                    prenorm=True,\n",
    "                    residual_in_fp32=self.residual_in_fp32,\n",
    "                    eps=self.norm.eps,\n",
    "                )    \n",
    "        hidden_states = self.mixer(hidden_states, inference_params=inference_params)\n",
    "        return hidden_states, residual\n",
    "\n",
    "    def allocate_inference_cache(self, batch_size, max_seqlen, dtype=None, **kwargs):\n",
    "        return self.mixer.allocate_inference_cache(batch_size, max_seqlen, dtype=dtype, **kwargs)\n",
    "\n",
    "\n",
    "def create_block(\n",
    "    d_model,\n",
    "    ssm_cfg=None,\n",
    "    norm_epsilon=1e-5,\n",
    "    drop_path=0.,\n",
    "    rms_norm=False,\n",
    "    residual_in_fp32=False,\n",
    "    fused_add_norm=False,\n",
    "    layer_idx=None,\n",
    "    device=None,\n",
    "    dtype=None,\n",
    "    if_bimamba=False,\n",
    "    bimamba_type=\"none\",\n",
    "    if_devide_out=False,\n",
    "    init_layer_scale=None,\n",
    "):\n",
    "    if if_bimamba:\n",
    "        bimamba_type = \"v1\"\n",
    "    if ssm_cfg is None:\n",
    "        ssm_cfg = {}\n",
    "    factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
    "    mixer_cls = partial(Mamba, layer_idx=layer_idx, bimamba_type=bimamba_type, if_devide_out=if_devide_out, init_layer_scale=init_layer_scale, **ssm_cfg, **factory_kwargs)\n",
    "    norm_cls = partial(\n",
    "        nn.LayerNorm if not rms_norm else RMSNorm, eps=norm_epsilon, **factory_kwargs\n",
    "    )\n",
    "    block = Block(\n",
    "        d_model,\n",
    "        mixer_cls,\n",
    "        norm_cls=norm_cls,\n",
    "        drop_path=drop_path,\n",
    "        fused_add_norm=fused_add_norm,\n",
    "        residual_in_fp32=residual_in_fp32,\n",
    "    )\n",
    "    block.layer_idx = layer_idx\n",
    "    return block\n",
    "\n",
    "\n",
    "# https://github.com/huggingface/transformers/blob/c28d04e9e252a1a099944e325685f14d242ecdcd/src/transformers/models/gpt2/modeling_gpt2.py#L454\n",
    "def _init_weights(\n",
    "    module,\n",
    "    n_layer,\n",
    "    initializer_range=0.02,  # Now only used for embedding layer.\n",
    "    rescale_prenorm_residual=True,\n",
    "    n_residuals_per_layer=1,  # Change to 2 if we have MLP\n",
    "):\n",
    "    if isinstance(module, nn.Linear):\n",
    "        if module.bias is not None:\n",
    "            if not getattr(module.bias, \"_no_reinit\", False):\n",
    "                nn.init.zeros_(module.bias)\n",
    "    elif isinstance(module, nn.Embedding):\n",
    "        nn.init.normal_(module.weight, std=initializer_range)\n",
    "\n",
    "    if rescale_prenorm_residual:\n",
    "        # Reinitialize selected weights subject to the OpenAI GPT-2 Paper Scheme:\n",
    "        #   > A modified initialization which accounts for the accumulation on the residual path with model depth. Scale\n",
    "        #   > the weights of residual layers at initialization by a factor of 1/√N where N is the # of residual layers.\n",
    "        #   >   -- GPT-2 :: https://openai.com/blog/better-language-models/\n",
    "        #\n",
    "        # Reference (Megatron-LM): https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/model/gpt_model.py\n",
    "        for name, p in module.named_parameters():\n",
    "            if name in [\"out_proj.weight\", \"fc2.weight\"]:\n",
    "                # Special Scaled Initialization --> There are 2 Layer Norms per Transformer Block\n",
    "                # Following Pytorch init, except scale by 1/sqrt(2 * n_layer)\n",
    "                # We need to reinit p since this code could be called multiple times\n",
    "                # Having just p *= scale would repeatedly scale it down\n",
    "                nn.init.kaiming_uniform_(p, a=math.sqrt(5))\n",
    "                with torch.no_grad():\n",
    "                    p /= math.sqrt(n_residuals_per_layer * n_layer)\n",
    "\n",
    "\n",
    "def segm_init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        trunc_normal_(m.weight, std=0.02)\n",
    "        if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "    elif isinstance(m, nn.Conv2d):\n",
    "        # NOTE conv was left to pytorch default in my original init\n",
    "        lecun_normal_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, (nn.LayerNorm, nn.GroupNorm, nn.BatchNorm2d)):\n",
    "        nn.init.zeros_(m.bias)\n",
    "        nn.init.ones_(m.weight)\n",
    "\n",
    "\n",
    "class VisionMamba(nn.Module):\n",
    "    def __init__(self, \n",
    "                 img_size=224, \n",
    "                 patch_size=16, \n",
    "                 stride=16,\n",
    "                 depth=24, \n",
    "                 embed_dim=192, \n",
    "                 channels=3, \n",
    "                 num_classes=1000,\n",
    "                 ssm_cfg=None, \n",
    "                 drop_rate=0.,\n",
    "                 drop_path_rate=0.1,\n",
    "                 norm_epsilon: float = 1e-5, \n",
    "                 rms_norm: bool = False, \n",
    "                 initializer_cfg=None,\n",
    "                 fused_add_norm=False,\n",
    "                 residual_in_fp32=False,\n",
    "                 device=None,\n",
    "                 dtype=None,\n",
    "                 ft_seq_len=None,\n",
    "                 pt_hw_seq_len=14,\n",
    "                 if_bidirectional=False,\n",
    "                 final_pool_type='none',\n",
    "                 if_abs_pos_embed=False,\n",
    "                 if_rope=False,\n",
    "                 if_rope_residual=False,\n",
    "                 flip_img_sequences_ratio=-1.,\n",
    "                 if_bimamba=False,\n",
    "                 bimamba_type=\"none\",\n",
    "                 if_cls_token=False,\n",
    "                 if_devide_out=False,\n",
    "                 init_layer_scale=None,\n",
    "                 use_double_cls_token=False,\n",
    "                 use_middle_cls_token=False,\n",
    "                 **kwargs):\n",
    "        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
    "        # add factory_kwargs into kwargs\n",
    "        kwargs.update(factory_kwargs) \n",
    "        super().__init__()\n",
    "        self.residual_in_fp32 = residual_in_fp32\n",
    "        self.fused_add_norm = fused_add_norm\n",
    "        self.if_bidirectional = if_bidirectional\n",
    "        self.final_pool_type = final_pool_type\n",
    "        self.if_abs_pos_embed = if_abs_pos_embed\n",
    "        self.if_rope = if_rope\n",
    "        self.if_rope_residual = if_rope_residual\n",
    "        self.flip_img_sequences_ratio = flip_img_sequences_ratio\n",
    "        self.if_cls_token = if_cls_token\n",
    "        self.use_double_cls_token = use_double_cls_token\n",
    "        self.use_middle_cls_token = use_middle_cls_token\n",
    "        self.num_tokens = 1 if if_cls_token else 0\n",
    "\n",
    "        # pretrain parameters\n",
    "        self.num_classes = num_classes\n",
    "        self.d_model = self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n",
    "\n",
    "        self.patch_embed = PatchEmbed(\n",
    "            img_size=img_size, patch_size=patch_size, stride=stride, in_chans=channels, embed_dim=embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "\n",
    "        if if_cls_token:\n",
    "            if use_double_cls_token:\n",
    "                self.cls_token_head = nn.Parameter(torch.zeros(1, 1, self.embed_dim))\n",
    "                self.cls_token_tail = nn.Parameter(torch.zeros(1, 1, self.embed_dim))\n",
    "                self.num_tokens = 2\n",
    "            else:\n",
    "                self.cls_token = nn.Parameter(torch.zeros(1, 1, self.embed_dim))\n",
    "                # self.num_tokens = 1\n",
    "            \n",
    "        if if_abs_pos_embed:\n",
    "            self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + self.num_tokens, self.embed_dim))\n",
    "            self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        if if_rope:\n",
    "            half_head_dim = embed_dim // 2\n",
    "            hw_seq_len = img_size // patch_size\n",
    "            self.rope = VisionRotaryEmbeddingFast(\n",
    "                dim=half_head_dim,\n",
    "                pt_seq_len=pt_hw_seq_len,\n",
    "                ft_seq_len=hw_seq_len\n",
    "            )\n",
    "        self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()\n",
    "\n",
    "\n",
    "        # TODO: release this comment\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
    "        # import ipdb;ipdb.set_trace()\n",
    "        inter_dpr = [0.0] + dpr\n",
    "        self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0. else nn.Identity()\n",
    "                # transformer blocks\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                create_block(\n",
    "                    embed_dim,\n",
    "                    ssm_cfg=ssm_cfg,\n",
    "                    norm_epsilon=norm_epsilon,\n",
    "                    rms_norm=rms_norm,\n",
    "                    residual_in_fp32=residual_in_fp32,\n",
    "                    fused_add_norm=fused_add_norm,\n",
    "                    layer_idx=i,\n",
    "                    if_bimamba=if_bimamba,\n",
    "                    bimamba_type=bimamba_type,\n",
    "                    drop_path=inter_dpr[i],\n",
    "                    if_devide_out=if_devide_out,\n",
    "                    init_layer_scale=init_layer_scale,\n",
    "                    **factory_kwargs,\n",
    "                )\n",
    "                for i in range(depth)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # output head\n",
    "        self.norm_f = (nn.LayerNorm if not rms_norm else RMSNorm)(\n",
    "            embed_dim, eps=norm_epsilon, **factory_kwargs\n",
    "        )\n",
    "\n",
    "        # self.pre_logits = nn.Identity()\n",
    "\n",
    "        # original init\n",
    "        self.patch_embed.apply(segm_init_weights)\n",
    "        self.head.apply(segm_init_weights)\n",
    "        if if_abs_pos_embed:\n",
    "            trunc_normal_(self.pos_embed, std=.02)\n",
    "        if if_cls_token:\n",
    "            if use_double_cls_token:\n",
    "                trunc_normal_(self.cls_token_head, std=.02)\n",
    "                trunc_normal_(self.cls_token_tail, std=.02)\n",
    "            else:\n",
    "                trunc_normal_(self.cls_token, std=.02)\n",
    "\n",
    "        # mamba init\n",
    "        self.apply(\n",
    "            partial(\n",
    "                _init_weights,\n",
    "                n_layer=depth,\n",
    "                **(initializer_cfg if initializer_cfg is not None else {}),\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "    def allocate_inference_cache(self, batch_size, max_seqlen, dtype=None, **kwargs):\n",
    "        return {\n",
    "            i: layer.allocate_inference_cache(batch_size, max_seqlen, dtype=dtype, **kwargs)\n",
    "            for i, layer in enumerate(self.layers)\n",
    "        }\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {\"pos_embed\", \"cls_token\", \"dist_token\", \"cls_token_head\", \"cls_token_tail\"}\n",
    "\n",
    "    @torch.jit.ignore()\n",
    "    def load_pretrained(self, checkpoint_path, prefix=\"\"):\n",
    "        _load_weights(self, checkpoint_path, prefix)\n",
    "\n",
    "    def forward_features(self, x, inference_params=None, if_random_cls_token_position=False, if_random_token_rank=False):\n",
    "        # taken from https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py\n",
    "        # with slight modifications to add the dist_token\n",
    "        x = self.patch_embed(x)\n",
    "        B, M, _ = x.shape\n",
    "\n",
    "        if self.if_cls_token:\n",
    "            if self.use_double_cls_token:\n",
    "                cls_token_head = self.cls_token_head.expand(B, -1, -1)\n",
    "                cls_token_tail = self.cls_token_tail.expand(B, -1, -1)\n",
    "                token_position = [0, M + 1]\n",
    "                x = torch.cat((cls_token_head, x, cls_token_tail), dim=1)\n",
    "                M = x.shape[1]\n",
    "            else:\n",
    "                if self.use_middle_cls_token:\n",
    "                    cls_token = self.cls_token.expand(B, -1, -1)\n",
    "                    token_position = M // 2\n",
    "                    # add cls token in the middle\n",
    "                    x = torch.cat((x[:, :token_position, :], cls_token, x[:, token_position:, :]), dim=1)\n",
    "                elif if_random_cls_token_position:\n",
    "                    cls_token = self.cls_token.expand(B, -1, -1)\n",
    "                    token_position = random.randint(0, M)\n",
    "                    x = torch.cat((x[:, :token_position, :], cls_token, x[:, token_position:, :]), dim=1)\n",
    "                    print(\"token_position: \", token_position)\n",
    "                else:\n",
    "                    cls_token = self.cls_token.expand(B, -1, -1)  # stole cls_tokens impl from Phil Wang, thanks\n",
    "                    token_position = 0\n",
    "                    x = torch.cat((cls_token, x), dim=1)\n",
    "                M = x.shape[1]\n",
    "\n",
    "        if self.if_abs_pos_embed:\n",
    "            # if new_grid_size[0] == self.patch_embed.grid_size[0] and new_grid_size[1] == self.patch_embed.grid_size[1]:\n",
    "            #     x = x + self.pos_embed\n",
    "            # else:\n",
    "            #     pos_embed = interpolate_pos_embed_online(\n",
    "            #                 self.pos_embed, self.patch_embed.grid_size, new_grid_size,0\n",
    "            #             )\n",
    "            x = x + self.pos_embed\n",
    "            x = self.pos_drop(x)\n",
    "\n",
    "        if if_random_token_rank:\n",
    "\n",
    "            # 生成随机 shuffle 索引\n",
    "            shuffle_indices = torch.randperm(M)\n",
    "\n",
    "            if isinstance(token_position, list):\n",
    "                print(\"original value: \", x[0, token_position[0], 0], x[0, token_position[1], 0])\n",
    "            else:\n",
    "                print(\"original value: \", x[0, token_position, 0])\n",
    "            print(\"original token_position: \", token_position)\n",
    "\n",
    "            # 执行 shuffle\n",
    "            x = x[:, shuffle_indices, :]\n",
    "\n",
    "            if isinstance(token_position, list):\n",
    "                # 找到 cls token 在 shuffle 之后的新位置\n",
    "                new_token_position = [torch.where(shuffle_indices == token_position[i])[0].item() for i in range(len(token_position))]\n",
    "                token_position = new_token_position\n",
    "            else:\n",
    "                # 找到 cls token 在 shuffle 之后的新位置\n",
    "                token_position = torch.where(shuffle_indices == token_position)[0].item()\n",
    "\n",
    "            if isinstance(token_position, list):\n",
    "                print(\"new value: \", x[0, token_position[0], 0], x[0, token_position[1], 0])\n",
    "            else:\n",
    "                print(\"new value: \", x[0, token_position, 0])\n",
    "            print(\"new token_position: \", token_position)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        if_flip_img_sequences = False\n",
    "        if self.flip_img_sequences_ratio > 0 and (self.flip_img_sequences_ratio - random.random()) > 1e-5:\n",
    "            x = x.flip([1])\n",
    "            if_flip_img_sequences = True\n",
    "\n",
    "        # mamba impl\n",
    "        residual = None\n",
    "        hidden_states = x\n",
    "        if not self.if_bidirectional:\n",
    "            for layer in self.layers:\n",
    "\n",
    "                if if_flip_img_sequences and self.if_rope:\n",
    "                    hidden_states = hidden_states.flip([1])\n",
    "                    if residual is not None:\n",
    "                        residual = residual.flip([1])\n",
    "\n",
    "                # rope about\n",
    "                if self.if_rope:\n",
    "                    hidden_states = self.rope(hidden_states)\n",
    "                    if residual is not None and self.if_rope_residual:\n",
    "                        residual = self.rope(residual)\n",
    "\n",
    "                if if_flip_img_sequences and self.if_rope:\n",
    "                    hidden_states = hidden_states.flip([1])\n",
    "                    if residual is not None:\n",
    "                        residual = residual.flip([1])\n",
    "\n",
    "                hidden_states, residual = layer(\n",
    "                    hidden_states, residual, inference_params=inference_params\n",
    "                )\n",
    "        else:\n",
    "            # get two layers in a single for-loop\n",
    "            for i in range(len(self.layers) // 2):\n",
    "                if self.if_rope:\n",
    "                    hidden_states = self.rope(hidden_states)\n",
    "                    if residual is not None and self.if_rope_residual:\n",
    "                        residual = self.rope(residual)\n",
    "\n",
    "                hidden_states_f, residual_f = self.layers[i * 2](\n",
    "                    hidden_states, residual, inference_params=inference_params\n",
    "                )\n",
    "                hidden_states_b, residual_b = self.layers[i * 2 + 1](\n",
    "                    hidden_states.flip([1]), None if residual == None else residual.flip([1]), inference_params=inference_params\n",
    "                )\n",
    "                hidden_states = hidden_states_f + hidden_states_b.flip([1])\n",
    "                residual = residual_f + residual_b.flip([1])\n",
    "\n",
    "        if not self.fused_add_norm:\n",
    "            if residual is None:\n",
    "                residual = hidden_states\n",
    "            else:\n",
    "                residual = residual + self.drop_path(hidden_states)\n",
    "            hidden_states = self.norm_f(residual.to(dtype=self.norm_f.weight.dtype))\n",
    "        else:\n",
    "            # Set prenorm=False here since we don't need the residual\n",
    "            fused_add_norm_fn = rms_norm_fn if isinstance(self.norm_f, RMSNorm) else layer_norm_fn\n",
    "            hidden_states = fused_add_norm_fn(\n",
    "                self.drop_path(hidden_states),\n",
    "                self.norm_f.weight,\n",
    "                self.norm_f.bias,\n",
    "                eps=self.norm_f.eps,\n",
    "                residual=residual,\n",
    "                prenorm=False,\n",
    "                residual_in_fp32=self.residual_in_fp32,\n",
    "            )\n",
    "\n",
    "        # return only cls token if it exists\n",
    "        if self.if_cls_token:\n",
    "            if self.use_double_cls_token:\n",
    "                return (hidden_states[:, token_position[0], :] + hidden_states[:, token_position[1], :]) / 2\n",
    "            else:\n",
    "                if self.use_middle_cls_token:\n",
    "                    return hidden_states[:, token_position, :]\n",
    "                elif if_random_cls_token_position:\n",
    "                    return hidden_states[:, token_position, :]\n",
    "                else:\n",
    "                    return hidden_states[:, token_position, :]\n",
    "\n",
    "        if self.final_pool_type == 'none':\n",
    "            return hidden_states[:, -1, :]\n",
    "        elif self.final_pool_type == 'mean':\n",
    "            return hidden_states.mean(dim=1)\n",
    "        elif self.final_pool_type == 'max':\n",
    "            return hidden_states\n",
    "        elif self.final_pool_type == 'all':\n",
    "            return hidden_states\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def forward(self, x, return_features=False, inference_params=None, if_random_cls_token_position=False, if_random_token_rank=False):\n",
    "        x = self.forward_features(x, inference_params, if_random_cls_token_position=if_random_cls_token_position, if_random_token_rank=if_random_token_rank)\n",
    "        if return_features:\n",
    "            return x\n",
    "        x = self.head(x)\n",
    "        if self.final_pool_type == 'max':\n",
    "            x = x.max(dim=1)[0]\n",
    "        return x\n",
    "\n",
    "\n",
    "@register_model\n",
    "def vim_tiny_patch16_224_bimambav2_final_pool_mean_abs_pos_embed_with_midclstok_div2(pretrained=False, **kwargs):\n",
    "    model = VisionMamba(\n",
    "        patch_size=16, embed_dim=192, depth=24, rms_norm=True, residual_in_fp32=True, fused_add_norm=True, final_pool_type='mean', if_abs_pos_embed=True, if_rope=False, if_rope_residual=False, bimamba_type=\"v2\", if_cls_token=True, if_devide_out=True, use_middle_cls_token=True, **kwargs)\n",
    "    model.default_cfg = _cfg()\n",
    "    if pretrained:\n",
    "        checkpoint = torch.hub.load_state_dict_from_url(\n",
    "            url=\"to.do\",\n",
    "            map_location=\"cpu\", check_hash=True\n",
    "        )\n",
    "        model.load_state_dict(checkpoint[\"model\"])\n",
    "    return model\n",
    "\n",
    "@register_model\n",
    "def vim_tiny_patch16_stride8_224_bimambav2_final_pool_mean_abs_pos_embed_with_midclstok_div2(pretrained=False, **kwargs):\n",
    "    model = VisionMamba(\n",
    "        patch_size=16, stride=8, embed_dim=192, depth=24, rms_norm=True, residual_in_fp32=True, fused_add_norm=True, final_pool_type='mean', if_abs_pos_embed=True, if_rope=False, if_rope_residual=False, bimamba_type=\"v2\", if_cls_token=True, if_devide_out=True, use_middle_cls_token=True, **kwargs)\n",
    "    model.default_cfg = _cfg()\n",
    "    if pretrained:\n",
    "        checkpoint = torch.hub.load_state_dict_from_url(\n",
    "            url=\"to.do\",\n",
    "            map_location=\"cpu\", check_hash=True\n",
    "        )\n",
    "        model.load_state_dict(checkpoint[\"model\"])\n",
    "    return model\n",
    "\n",
    "@register_model\n",
    "def vim_small_patch16_224_bimambav2_final_pool_mean_abs_pos_embed_with_midclstok_div2(pretrained=False, **kwargs):\n",
    "    model = VisionMamba(\n",
    "        patch_size=16, embed_dim=384, depth=24, rms_norm=True, residual_in_fp32=True, fused_add_norm=True, final_pool_type='mean', if_abs_pos_embed=True, if_rope=False, if_rope_residual=False, bimamba_type=\"v2\", if_cls_token=True, if_devide_out=True, use_middle_cls_token=True, **kwargs)\n",
    "    model.default_cfg = _cfg()\n",
    "    if pretrained:\n",
    "        checkpoint = torch.hub.load_state_dict_from_url(\n",
    "            url=\"to.do\",\n",
    "            map_location=\"cpu\", check_hash=True\n",
    "        )\n",
    "        model.load_state_dict(checkpoint[\"model\"])\n",
    "    return model\n",
    "\n",
    "@register_model\n",
    "def vim_small_patch16_stride8_224_bimambav2_final_pool_mean_abs_pos_embed_with_midclstok_div2(pretrained=False, **kwargs):\n",
    "    model = VisionMamba(\n",
    "        patch_size=16, stride=8, embed_dim=384, depth=24, rms_norm=True, residual_in_fp32=True, fused_add_norm=True, final_pool_type='mean', if_abs_pos_embed=True, if_rope=False, if_rope_residual=False, bimamba_type=\"v2\", if_cls_token=True, if_devide_out=True, use_middle_cls_token=True, **kwargs)\n",
    "    model.default_cfg = _cfg()\n",
    "    if pretrained:\n",
    "        checkpoint = torch.hub.load_state_dict_from_url(\n",
    "            url=\"to.do\",\n",
    "            map_location=\"cpu\", check_hash=True\n",
    "        )\n",
    "        model.load_state_dict(checkpoint[\"model\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_model\n",
    "def vim_tiny_patch4_32_bimambav2_final_pool_mean_abs_pos_embed_with_midclstok_div2(pretrained=False, **kwargs):\n",
    "    model = VisionMamba(\n",
    "        img_size=32,\n",
    "        patch_size=4,\n",
    "        stride=4,\n",
    "        embed_dim=168,  # Reduced embedding dimension\n",
    "        depth=8,  # Reduced depth\n",
    "        rms_norm=True,\n",
    "        residual_in_fp32=True,\n",
    "        fused_add_norm=True,\n",
    "        final_pool_type='mean',\n",
    "        if_abs_pos_embed=True,\n",
    "        if_rope=False,\n",
    "        if_rope_residual=False,\n",
    "        bimamba_type=\"v2\",\n",
    "        if_cls_token=True,\n",
    "        if_devide_out=True,\n",
    "        use_middle_cls_token=True,\n",
    "        **kwargs\n",
    "    )\n",
    "    model.default_cfg = _cfg()\n",
    "    if pretrained:\n",
    "        checkpoint = torch.hub.load_state_dict_from_url(\n",
    "            url=\"to.do\",\n",
    "            map_location=\"cpu\", check_hash=True\n",
    "        )\n",
    "        model.load_state_dict(checkpoint[\"model\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import Compose, RandomCrop, RandomHorizontalFlip, ToTensor, Normalize, RandAugment\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Define the training loop\n",
    "def train(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "        predicted_labels.extend(predicted.cpu().numpy())\n",
    "\n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    epoch_acc = 100.0 * correct / total\n",
    "    epoch_f1 = f1_score(true_labels, predicted_labels, average='macro')\n",
    "\n",
    "    return epoch_loss, epoch_acc, epoch_f1\n",
    "\n",
    "# Define the validation loop\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "            predicted_labels.extend(predicted.cpu().numpy())\n",
    "\n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    epoch_acc = 100.0 * correct / total\n",
    "    epoch_f1 = f1_score(true_labels, predicted_labels, average='macro')\n",
    "\n",
    "    return epoch_loss, epoch_acc, epoch_f1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CIFAR-100 dataset with augmentations\n",
    "transform_train = Compose([\n",
    "    RandomCrop(32, padding=4),\n",
    "    RandomHorizontalFlip(),\n",
    "    RandAugment(num_ops=2, magnitude=9),  # Add RandAugment\n",
    "    ToTensor(),\n",
    "    Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))\n",
    "])\n",
    "transform_test = Compose([\n",
    "    ToTensor(),\n",
    "    Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "trainloader = DataLoader(trainset, batch_size=64, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "testloader = DataLoader(testset, batch_size=64, shuffle=False, num_workers=2)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model, loss function, optimizer, and scheduler\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# Set up model, loss function, and optimizer\n",
    "model = vim_tiny_patch4_32_bimambav2_final_pool_mean_abs_pos_embed_with_midclstok_div2()\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in the model: 1,958,536\n"
     ]
    }
   ],
   "source": [
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Number of parameters in the model: {num_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "checkpoint_dir = \"checkpoints\"\n",
    "checkpoint_dir = 'checkpoints'\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_interval = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "val_losses = []\n",
    "val_accs = []\n",
    "val_f1s = []\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "start_time = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc, train_f1 = train(model, trainloader, criterion, optimizer, device)\n",
    "    val_loss, val_acc, val_f1 = validate(model, testloader, criterion, device)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accs.append(val_acc)\n",
    "    val_f1s.append(val_f1)\n",
    "\n",
    "    # print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Train F1: {train_f1:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%, Val F1: {val_f1:.4f}')\n",
    "\n",
    "    # Update the learning rate scheduler\n",
    "    # Save checkpoint every 10 epochs\n",
    "    # if (epoch + 1) % checkpoint_interval == 0:\n",
    "    #     checkpoint_path = os.path.join(checkpoint_dir, f'checkpoint_epoch_{epoch+1}.pt')\n",
    "    #     torch.save({\n",
    "    #         'epoch': epoch + 1,\n",
    "    #         'model_state_dict': model.state_dict(),\n",
    "    #         'optimizer_state_dict': optimizer.state_dict(),\n",
    "    #         'train_loss': train_loss,\n",
    "    #         'train_acc': train_acc,\n",
    "    #         'train_f1': train_f1,\n",
    "    #         'val_loss': val_loss,\n",
    "    #         'val_acc': val_acc,\n",
    "    #         'val_f1': val_f1\n",
    "    #     }, checkpoint_path)\n",
    "    #     print(f'Checkpoint saved at epoch {epoch+1}')\n",
    "end_time = time.time()\n",
    "total_training_time = end_time - start_time\n",
    "print(total_training_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "data = {\n",
    "    \"val_losses\": val_losses,\n",
    "    \"val_accs\": val_accs,\n",
    "    \"val_f1s\": val_f1s,\n",
    "    'time':total_training_time\n",
    "}\n",
    "\n",
    "# Save the data to a JSON file\n",
    "with open(\"val_vim.json\", \"w\") as file:\n",
    "    json.dump(data, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(val_losses, 'bo-')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation Loss')\n",
    "plt.title('Validation Loss vs. Epoch')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(val_accs, 'go-')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.title('Validation Accuracy vs. Epoch')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(val_f1s, 'ro-')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation F1 Score')\n",
    "plt.title('Validation F1 Score vs. Epoch')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mamba-classification",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
